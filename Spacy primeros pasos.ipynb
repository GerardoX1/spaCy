{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial uso de spacy\n",
    "\n",
    "Todo lo que necesitas saber de spacy esta aqui: https://spacy.io/usage/spacy-101 cargamos el packete de spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Para generar un objeto de analisis se tiene que crear un objeto del modelo. Existen varias formas de cargar un modelo como son con el enlace de acceso directo del modelo , el nombre del paquete o la ruta al directorio de datos:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('es_core_news_sm')\n",
    "\n",
    "# nlp = spacy.load('en')                       # load model with shortcut link \"en\"\n",
    "# nlp = spacy.load('en_core_web_sm')           # load model package \"en_core_web_sm\"\n",
    "# nlp = spacy.load('/path/to/en_core_web_sm')  # load package from a directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se introduce el texto que se va a analizar de la siguiente forma:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"\"\"\n",
    "El accidente de Chernóbil fue un accidente nuclear sucedido el 26 de abril de 1986 en la central nuclear Vladímir Ilich Lenin, ubicada en el norte de Ucrania, que en ese momento pertenecía a la Unión de Repúblicas Socialistas Soviéticas, a 3 km de la ciudad de Prípiat, a 18  km de la ciudad de Chernóbil y a 17 km de la frontera con Bielorrusia.\n",
    "\n",
    "Considerado, junto con el accidente nuclear de Fukushima I en Japón en 2011, como el más grave en la Escala Internacional de Accidentes Nucleares (accidente mayor, nivel 7), y suele ser incluido entre los grandes desastres medioambientales de la historia.\n",
    "\n",
    "Las causas y desarrollo del accidente son objeto de controversias. Existe un consenso general en que desde el día anterior se venía realizando una prueba que requería reducir la potencia, durante la cual se produjeron una serie de desequilibrios en el reactor 4 de esta central nuclear, que desembocaron en el sobrecalentamiento descontrolado del núcleo del reactor nuclear y en una o dos explosiones sucesivas, seguidas de un incendio generalizado, que volaron la tapa del reactor de 1200 toneladas y expulsaron grandes cantidades de materiales radiactivos a la atmósfera, formando una nube radiactiva que se extendió por Europa y América del Norte.\n",
    "\n",
    "La cantidad de dióxido de uranio, carburo de boro, óxido de europio, erbio, aleaciones de circonio y grafito expulsados,6​ materiales radiactivos y/o tóxicos, que se estimó fue unas 500 veces mayor que el liberado por la bomba atómica arrojada en Hiroshima en 1945, causó la muerte de 31 personas en las siguientes dos semanas y llevó al Gobierno de la Unión Soviética a la evacuación de urgencia de 116 000 personas, provocando una alarma internacional al detectarse radiactividad en al menos 13 países de Europa central y oriental.\n",
    "\n",
    "Después del accidente, se inició un proceso masivo de descontaminación, contención y mitigación que desempeñaron aproximadamente 600 000 personas denominadas liquidadores en las zonas circundantes al lugar del accidente y se aisló un área de 30 km de radio alrededor de la central nuclear conocida como zona de alienación, que sigue aún vigente. Solo una pequeña parte de los liquidadores se vieron expuestos a altos índices de radiactividad.\n",
    "\n",
    "Dos empleados de la planta murieron como consecuencia directa de la explosión y otros 29 fallecieron en los tres meses siguientes. Unas 1000 personas recibieron grandes dosis de radiación durante el primer día después del accidente, 200 000 personas recibieron alrededor de 100 mSv, 20 000 cerca de 250 mSv y algunos 500 mSv. En total, 600 000 personas recibieron dosis de radiación por los trabajos de descontaminación posteriores al accidente. 5 000 000 de personas vivieron en áreas contaminadas y 400 000 en áreas gravemente contaminadas. Hasta hoy no existen trabajos concluyentes sobre la incidencia real, y no teórica, de este accidente en la mortalidad de la población.\n",
    "\n",
    "Tras prolongadas negociaciones con el Gobierno ucraniano, la comunidad internacional financió los costes del cierre definitivo de la central, completado el 15 de diciembre de 2000. Inmediatamente después del accidente se construyó un «sarcófago», para cubrir el reactor y aislar el interior del exterior, que se vio degradado con el paso del tiempo por diversos fenómenos naturales, y por las dificultades de construirlo en un ambiente de alta radiación, por lo que corría riesgo de degradarse seriamente. En 2004, se inició la construcción de un nuevo sarcófago para el reactor. El resto de reactores de la central están inactivos.\n",
    "\n",
    "En noviembre de 2016, treinta años después de la tragedia, se inauguró un nuevo sarcófago, al que se denominó «nuevo sarcófago seguro» (NSC, por sus siglas en inglés), una estructura móvil, la mayor construida hasta la fecha en el mundo, en forma de arco de 110 metros de alto, 150 de ancho y 256 de largo y más de 30 000 toneladas de peso. Se construyó a 180 metros del reactor y luego se ubicó sobre él, desplazándolo mediante un sofisticado sistema de raíles. Se construyó con características que le dieron una durabilidad estimada de más de cien años. El coste final de la estructura fue de 1500 millones de euros, financiado por el Banco Europeo para la Reconstrucción y el Desarrollo (BERD), junto a la colaboración de 28 países que aportaron 1417 millones de euros, y construido por la empresa francesa Novarka. La estructura está equipada con grúas controladas a distancia con el objetivo de ir desmontando la antigua estructura.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Analizando el texto con Spacy la variable doc contiene una version analizada del texto, aunque aparentemente se mantuvo sin cambios se generaron atributos y propiedades partidas del analisis del texto*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "El accidente de Chernóbil fue un accidente nuclear sucedido el 26 de abril de 1986 en la central nuclear Vladímir Ilich Lenin, ubicada en el norte de Ucrania, que en ese momento pertenecía a la Unión de Repúblicas Socialistas Soviéticas, a 3 km de la ciudad de Prípiat, a 18  km de la ciudad de Chernóbil y a 17 km de la frontera con Bielorrusia.\n",
      "\n",
      "Considerado, junto con el accidente nuclear de Fukushima I en Japón en 2011, como el más grave en la Escala Internacional de Accidentes Nucleares (accidente mayor, nivel 7), y suele ser incluido entre los grandes desastres medioambientales de la historia.\n",
      "\n",
      "Las causas y desarrollo del accidente son objeto de controversias. Existe un consenso general en que desde el día anterior se venía realizando una prueba que requería reducir la potencia, durante la cual se produjeron una serie de desequilibrios en el reactor 4 de esta central nuclear, que desembocaron en el sobrecalentamiento descontrolado del núcleo del reactor nuclear y en una o dos explosiones sucesivas, seguidas de un incendio generalizado, que volaron la tapa del reactor de 1200 toneladas y expulsaron grandes cantidades de materiales radiactivos a la atmósfera, formando una nube radiactiva que se extendió por Europa y América del Norte.\n",
      "\n",
      "La cantidad de dióxido de uranio, carburo de boro, óxido de europio, erbio, aleaciones de circonio y grafito expulsados,6​ materiales radiactivos y/o tóxicos, que se estimó fue unas 500 veces mayor que el liberado por la bomba atómica arrojada en Hiroshima en 1945, causó la muerte de 31 personas en las siguientes dos semanas y llevó al Gobierno de la Unión Soviética a la evacuación de urgencia de 116 000 personas, provocando una alarma internacional al detectarse radiactividad en al menos 13 países de Europa central y oriental.\n",
      "\n",
      "Después del accidente, se inició un proceso masivo de descontaminación, contención y mitigación que desempeñaron aproximadamente 600 000 personas denominadas liquidadores en las zonas circundantes al lugar del accidente y se aisló un área de 30 km de radio alrededor de la central nuclear conocida como zona de alienación, que sigue aún vigente. Solo una pequeña parte de los liquidadores se vieron expuestos a altos índices de radiactividad.\n",
      "\n",
      "Dos empleados de la planta murieron como consecuencia directa de la explosión y otros 29 fallecieron en los tres meses siguientes. Unas 1000 personas recibieron grandes dosis de radiación durante el primer día después del accidente, 200 000 personas recibieron alrededor de 100 mSv, 20 000 cerca de 250 mSv y algunos 500 mSv. En total, 600 000 personas recibieron dosis de radiación por los trabajos de descontaminación posteriores al accidente. 5 000 000 de personas vivieron en áreas contaminadas y 400 000 en áreas gravemente contaminadas. Hasta hoy no existen trabajos concluyentes sobre la incidencia real, y no teórica, de este accidente en la mortalidad de la población.\n",
      "\n",
      "Tras prolongadas negociaciones con el Gobierno ucraniano, la comunidad internacional financió los costes del cierre definitivo de la central, completado el 15 de diciembre de 2000. Inmediatamente después del accidente se construyó un «sarcófago», para cubrir el reactor y aislar el interior del exterior, que se vio degradado con el paso del tiempo por diversos fenómenos naturales, y por las dificultades de construirlo en un ambiente de alta radiación, por lo que corría riesgo de degradarse seriamente. En 2004, se inició la construcción de un nuevo sarcófago para el reactor. El resto de reactores de la central están inactivos.\n",
      "\n",
      "En noviembre de 2016, treinta años después de la tragedia, se inauguró un nuevo sarcófago, al que se denominó «nuevo sarcófago seguro» (NSC, por sus siglas en inglés), una estructura móvil, la mayor construida hasta la fecha en el mundo, en forma de arco de 110 metros de alto, 150 de ancho y 256 de largo y más de 30 000 toneladas de peso. Se construyó a 180 metros del reactor y luego se ubicó sobre él, desplazándolo mediante un sofisticado sistema de raíles. Se construyó con características que le dieron una durabilidad estimada de más de cien años. El coste final de la estructura fue de 1500 millones de euros, financiado por el Banco Europeo para la Reconstrucción y el Desarrollo (BERD), junto a la colaboración de 28 países que aportaron 1417 millones de euros, y construido por la empresa francesa Novarka. La estructura está equipada con grúas controladas a distancia con el objetivo de ir desmontando la antigua estructura.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(texto)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización en oraciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a continuacion se muestran el codigo para tokenizar oraciones, utilizamos el metodo numerate para enumerar los items obtenidos en este caso los tokens. cuando se tokeniza una oracion tecnicamente lo que el programa realiza es separar por puntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \n",
      "El accidente de Chernóbil fue un accidente nuclear sucedido el 26 de abril de 1986 en la central nuclear Vladímir Ilich Lenin, ubicada en el norte de Ucrania, que en ese momento pertenecía a la Unión de Repúblicas Socialistas Soviéticas, a 3 km de la ciudad de Prípiat, a 18  km de la ciudad de Chernóbil y a 17 km de la frontera con Bielorrusia.\n",
      "\n",
      "\n",
      "1: Considerado, junto con el accidente nuclear de Fukushima I en Japón en 2011, como el más grave en la Escala Internacional de Accidentes Nucleares (accidente mayor, nivel 7), y suele ser incluido entre los grandes desastres medioambientales de la historia.\n",
      "\n",
      "\n",
      "2: Las causas y desarrollo del accidente son objeto de controversias.\n",
      "3: Existe un consenso general en que desde el día anterior se venía realizando una prueba que requería reducir la potencia, durante la cual se produjeron una serie de desequilibrios en el reactor 4 de esta central nuclear, que desembocaron en el sobrecalentamiento descontrolado del núcleo del reactor nuclear y en una o dos explosiones sucesivas, seguidas de un incendio generalizado, que volaron la tapa del reactor de 1200 toneladas y expulsaron grandes cantidades de materiales radiactivos a la atmósfera, formando una nube radiactiva que se extendió por Europa y América del Norte.\n",
      "\n",
      "\n",
      "4: La cantidad de dióxido de uranio, carburo de boro, óxido de europio, erbio, aleaciones de circonio y grafito expulsados,6​ materiales radiactivos y/o tóxicos, que se estimó fue unas 500 veces mayor que el liberado por la bomba atómica arrojada en Hiroshima en 1945, causó la muerte de 31 personas en las siguientes dos semanas y llevó al Gobierno de la Unión Soviética a la evacuación de urgencia de 116 000 personas, provocando una alarma internacional al detectarse radiactividad en al menos 13 países de Europa central y oriental.\n",
      "\n",
      "\n",
      "5: Después del accidente, se inició un proceso masivo de descontaminación, contención y mitigación que desempeñaron aproximadamente 600 000 personas denominadas liquidadores en las zonas circundantes al lugar del accidente y se aisló un área de 30 km de radio alrededor de la central nuclear conocida como zona de alienación, que sigue aún vigente.\n",
      "6: Solo una pequeña parte de los liquidadores se vieron expuestos a altos índices de radiactividad.\n",
      "\n",
      "\n",
      "7: Dos empleados de la planta murieron como consecuencia directa de la explosión y otros 29 fallecieron en los tres meses siguientes.\n",
      "8: Unas 1000 personas recibieron grandes dosis de radiación durante el primer día después del accidente, 200 000 personas recibieron alrededor de 100 mSv, 20 000 cerca de 250 mSv y algunos 500 mSv.\n",
      "9: En total, 600 000 personas recibieron dosis de radiación por los trabajos de descontaminación posteriores al accidente.\n",
      "10: 5 000 000 de personas vivieron en áreas contaminadas y 400 000 en áreas gravemente contaminadas.\n",
      "11: Hasta hoy no existen trabajos concluyentes sobre la incidencia real, y no teórica, de este accidente en la mortalidad de la población.\n",
      "\n",
      "\n",
      "12: Tras prolongadas negociaciones con el Gobierno ucraniano, la comunidad internacional financió los costes del cierre definitivo de la central, completado el 15 de diciembre de 2000.\n",
      "13: Inmediatamente después del accidente se construyó un «sarcófago», para cubrir el reactor y aislar el interior del exterior, que se vio degradado con el paso del tiempo por diversos fenómenos naturales, y por las dificultades de construirlo en un ambiente de alta radiación, por lo que corría riesgo de degradarse seriamente.\n",
      "14: En 2004, se inició la construcción de un nuevo sarcófago para el reactor.\n",
      "15: El resto de reactores de la central están inactivos.\n",
      "\n",
      "\n",
      "16: En noviembre de 2016, treinta años después de la tragedia, se inauguró un nuevo sarcófago, al que se denominó «nuevo sarcófago seguro» (NSC, por sus siglas en inglés), una estructura móvil, la mayor construida hasta la fecha en el mundo, en forma de arco de 110 metros de alto, 150 de ancho y 256 de largo y más de 30 000 toneladas de peso.\n",
      "17: Se construyó a 180 metros del reactor y luego se ubicó sobre él, desplazándolo mediante un sofisticado sistema de raíles.\n",
      "18: Se construyó con características que le dieron una durabilidad estimada de más de cien años.\n",
      "19: El coste final de la estructura fue de 1500 millones de euros, financiado por el Banco Europeo para la Reconstrucción y el Desarrollo (BERD), junto a la colaboración de 28 países que aportaron 1417 millones de euros, y construido por la empresa francesa Novarka.\n",
      "20: La estructura está equipada con grúas controladas a distancia con el objetivo de ir desmontando la antigua estructura.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Para tokenizar oraciones\n",
    "\n",
    "for num,oracion in enumerate(doc.sents):\n",
    "    print(f'{num}: {oracion}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comienzan a notarse las deficiencias que existen en modelos en español, puesto que las oraciones dimensiones homogeneas, posteriormente resolveremos este problema mediante entrenamiento, mientras imaginemos que todo va bien.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización de palabras\n",
    "Para analizar elemento por elemento lo unico que hacemos es leer todo el documento y mandar dato por dato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "El\n",
      "accidente\n",
      "de\n",
      "Chernóbil\n",
      "fue\n",
      "un\n",
      "accidente\n",
      "nuclear\n",
      "sucedido\n"
     ]
    }
   ],
   "source": [
    "#Para tokenizar palabras\n",
    "for palabra in doc[:10]:\n",
    "    print(palabra.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'El', 'accidente', 'de', 'Chernóbil', 'fue', 'un', 'accidente', 'nuclear', 'sucedido', 'el', '26', 'de', 'abril', 'de', '1986', 'en', 'la', 'central', 'nuclear', 'Vladímir', 'Ilich', 'Lenin', ',', 'ubicada', 'en', 'el', 'norte', 'de', 'Ucrania']\n"
     ]
    }
   ],
   "source": [
    "# Para hacer una lista de tokens\n",
    "lista_palabras=[palabra.text.replace('\\n','') for palabra in doc[:30]]\n",
    "print(lista_palabras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forma de las palabras\n",
    "\n",
    "*la forma de las palabras es algo muy simple se coloca una X mayuscula cuando esta en mayuscula la letra y en minuscula en el caso contrario, dando estructuras de Xxxxx finalmente se muestra si esta palabra contiene caracteres alfanumericos con un True de lo contrario un false*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " False\n",
      "El Xx True\n",
      "accidente xxxx True\n",
      "de xx True\n",
      "Chernóbil Xxxxx True\n",
      "fue xxx True\n",
      "un xx True\n",
      "accidente xxxx True\n",
      "nuclear xxxx True\n",
      "sucedido xxxx True\n",
      "el xx True\n",
      "26 dd False\n",
      "de xx True\n",
      "abril xxxx True\n",
      "de xx True\n",
      "1986 dddd False\n",
      "en xx True\n",
      "la xx True\n",
      "central xxxx True\n",
      "nuclear xxxx True\n"
     ]
    }
   ],
   "source": [
    "#La forma de las palabras y saber si la palabra contiene caracteres alfanumericos,si si devuelve True.\n",
    "for palabra in doc[:20]:\n",
    "    print(palabra.text,palabra.shape_,palabra.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etiquetando partes de un discurso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Una vez realizado tu pipeline y procesada la informacion con los elementos ya tokenizados puedes ver sus atributos instancias que te ayudaran a definir ciertos parametros como el tipo de elemento en el lenguaje, la relación con otros tokens entre muchas otras cualidades con las que cuenta el token, a continuación se muestran las mas tipicas para mas información revisar https://spacy.io/api/token#attributes \n",
    "\n",
    "pos_ = tipo de palabra por ejemplo sustantivo, verbo, preposicion\n",
    "\n",
    "tag_ = revision minuciosa de la cualidad anterior espesificando parametros dentro de la categoria\n",
    "\n",
    "dep_ = relacion de dependencia sintactica\n",
    "*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n', 'SPACE', '_SP', '')\n",
      "('El', 'DET', 'DET__Definite=Def|Gender=Masc|Number=Sing|PronType=Art', 'det')\n",
      "('accidente', 'NOUN', 'NOUN__Gender=Masc|Number=Sing', 'nsubj')\n",
      "('de', 'ADP', 'ADP__AdpType=Prep', 'case')\n",
      "('Chernóbil', 'PROPN', 'PROPN___', 'nmod')\n",
      "('fue', 'AUX', 'AUX__Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin', 'cop')\n",
      "('un', 'DET', 'DET__Definite=Ind|Gender=Masc|Number=Sing|PronType=Art', 'det')\n",
      "('accidente', 'NOUN', 'NOUN__Gender=Masc|Number=Sing', 'ROOT')\n",
      "('nuclear', 'ADJ', 'ADJ__Number=Sing', 'amod')\n",
      "('sucedido', 'ADJ', 'ADJ__Gender=Masc|Number=Sing|VerbForm=Part', 'amod')\n"
     ]
    }
   ],
   "source": [
    "for palabra in doc[:10]:\n",
    "    print((palabra.text,palabra.pos_,palabra.tag_,palabra.dep_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Con el siguiente metodo nos devolvera la explicacion de un termino \n",
    "para mas elementos y entender mejor revisar https://github.com/explosion/spaCy/blob/master/spacy/glossary.py *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adjectival modifier'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('amod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencia Sintactica\n",
    "El visualizador de dependencias dep muestra etiquetas de parte del discurso y dependencias sintácticas.\n",
    "se coloca el texto a analizar y con el displacy.render podemos observar de forma\n",
    "grafica como se relacionan estos elementos de forma grafica\n",
    "\n",
    "más informacion en https://spacy.io/api/top-level#displacy_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"es\" id=\"7271f6d9819a40178b854742213fb405-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Miriam</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">ama</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Luis</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7271f6d9819a40178b854742213fb405-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7271f6d9819a40178b854742213fb405-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7271f6d9819a40178b854742213fb405-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7271f6d9819a40178b854742213fb405-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7271f6d9819a40178b854742213fb405-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7271f6d9819a40178b854742213fb405-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ex=nlp('Miriam ama a Luis')\n",
    "displacy.render(ex,style='dep',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos el mismo analisis con un texto mas grande y el resultado fue el siguiente (para mejorar la visualizacion de los datos usaremos un stilo diferente):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"></br>\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    El accidente de Chernóbil\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " fue un accidente nuclear sucedido el 26 de abril de 1986 en la central nuclear \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Vladímir Ilich Lenin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ", ubicada en el norte de \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Ucrania\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", que en ese momento pertenecía a la \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Unión de Repúblicas Socialistas Soviéticas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", a 3 km de la ciudad de \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Prípiat\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", a 18  km de la ciudad de \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Chernóbil\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " y a 17 km de la frontera con \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Bielorrusia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ".</br></br>\n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Considerado\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", junto con el accidente nuclear de \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Fukushima I\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " en \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Japón\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " en 2011, como el más grave en la \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Escala Internacional de Accidentes Nucleares\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " (accidente mayor, nivel 7), y suele ser incluido entre los grandes desastres medioambientales de la historia.</br></br>\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Las causas y desarrollo del accidente\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " son objeto de controversias. \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Existe un\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " consenso general en que desde el día anterior se venía realizando una prueba que requería reducir la potencia, durante la cual se produjeron una serie de desequilibrios en el reactor 4 de esta central nuclear, que desembocaron en el sobrecalentamiento descontrolado del núcleo del reactor nuclear y en una o dos explosiones sucesivas, seguidas de un incendio generalizado, que volaron la tapa del reactor de 1200 toneladas y expulsaron grandes cantidades de materiales radiactivos a la atmósfera, formando una nube radiactiva que se extendió por \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Europa\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " y \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    América del Norte\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ".</br></br>La cantidad de dióxido de uranio, carburo de boro, óxido de europio, erbio, aleaciones de circonio y grafito expulsados,6​ materiales radiactivos y/o tóxicos, que se estimó fue unas 500 veces mayor que el liberado por la bomba atómica arrojada en \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Hiroshima\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " en 1945, causó la muerte de 31 personas en las siguientes dos semanas y llevó al \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Gobierno de la Unión Soviética\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " a la evacuación de urgencia de 116 000 personas, provocando una alarma internacional al detectarse radiactividad en al menos 13 países de \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Europa\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " central y oriental.</br></br>\n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Después\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " del accidente, se inició un proceso masivo de descontaminación, contención y mitigación que desempeñaron aproximadamente 600 000 personas denominadas liquidadores en las zonas circundantes al lugar del accidente y se aisló un área de 30 km de radio alrededor de la central nuclear conocida como zona de alienación, que sigue aún vigente. \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Solo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " una pequeña parte de los liquidadores se vieron expuestos a altos índices de radiactividad.</br></br>\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Dos empleados de la planta\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " murieron como consecuencia directa de la explosión y otros 29 fallecieron en los tres meses siguientes. \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Unas 1000\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " personas recibieron grandes dosis de radiación durante el primer día después del accidente, 200 000 personas recibieron alrededor de 100 mSv, 20 000 cerca de 250 mSv y algunos 500 mSv. \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    En total\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       ", 600 000 personas recibieron dosis de radiación por los trabajos de descontaminación posteriores al accidente. 5 000 000 de personas vivieron en áreas contaminadas y 400 000 en áreas gravemente contaminadas. \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Hasta hoy no existen trabajos concluyentes sobre la incidencia real\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       ", y no teórica, de este accidente en la mortalidad de la población.</br></br>Tras prolongadas negociaciones con el \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Gobierno ucraniano\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", la comunidad internacional financió los costes del cierre definitivo de la central, completado el 15 de diciembre de 2000. \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Inmediatamente después del accidente\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " se construyó un «sarcófago\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    »\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       ", para cubrir el reactor y aislar el interior del exterior, que se vio degradado con el paso del tiempo por diversos fenómenos naturales, y por las dificultades de construirlo en un ambiente de alta radiación, por lo que corría riesgo de degradarse seriamente. En 2004, se inició la construcción de un nuevo sarcófago para el reactor. \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    El resto de reactores\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " de la central están inactivos.</br></br>\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    En noviembre de 2016\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       ", treinta años después de la tragedia, se inauguró un nuevo sarcófago, al que se denominó «nuevo sarcófago seguro\n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    »\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    NSC\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       ", por sus siglas en inglés), una estructura móvil, la mayor construida hasta la fecha en el mundo, en forma de arco de 110 metros de alto, 150 de ancho y 256 de largo y más de 30 000 toneladas de peso. Se construyó a 180 metros del reactor y luego se ubicó sobre él, desplazándolo mediante un sofisticado sistema de raíles. Se construyó con características que le dieron una durabilidad estimada de más de cien años. \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    El coste final de la estructura\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " fue de 1500 millones de euros, financiado por el \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Banco Europeo para la Reconstrucción\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " y el Desarrollo (\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    BERD\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "), junto a la colaboración de 28 países que aportaron 1417 millones de euros, y construido por la empresa francesa \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Novarka\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    La estructura está equipada\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " con grúas controladas a distancia con el objetivo de ir desmontando la antigua estructura.\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc,style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lematización\n",
    "\n",
    "La lematizacion asigna las formas basicas a las palabras es decir si tenemos las palabras juego, jugaremos, jugaron , jugabamos , la forma base es el verbo Jugar, para mas información https://spacy.io/api/lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estudiar estudiar VERB\n",
      "estudioso estudioso ADJ\n",
      "estudia estudiar VERB\n",
      "estudio estudiar NOUN\n",
      "estudiando estudiar VERB\n",
      "estudiante estudiante ADV\n",
      "estudiaba estudiar VERB\n",
      "estudió estudiar VERB\n"
     ]
    }
   ],
   "source": [
    "docex=nlp('estudiar estudioso estudia estudio estudiando estudiante estudiaba estudió')\n",
    "for w in docex:\n",
    "    print(w.text,w.lemma_,w.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconocimientos de Entidades\n",
    "\n",
    "Los modelos de entrenamiento del Corpus de ontoNotes 5 soportan varias entidades como:\n",
    "\n",
    "PERSON Personas, incluso ficticias. NORP Nacionalidades o grupos religiosos o políticos. FAC Edificios, aeropuertos, carreteras, puentes, etc. ORG Empresas, agencias, instituciones, etc. GPE Países, ciudades, estados. LOC Ubicaciones no GPE, sierras, cuerpos de agua.\n",
    "\n",
    "para mas información revisar: https://spacy.io/api/annotation buscar \"Named Entity Recognition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El accidente de Chernóbil MISC\n",
      "Vladímir Ilich Lenin PER\n",
      "Ucrania LOC\n",
      "Unión de Repúblicas Socialistas Soviéticas LOC\n",
      "Prípiat LOC\n",
      "Chernóbil LOC\n",
      "Bielorrusia LOC\n",
      "Considerado LOC\n",
      "Fukushima I PER\n",
      "Japón LOC\n",
      "Escala Internacional de Accidentes Nucleares ORG\n",
      "Las causas y desarrollo del accidente MISC\n",
      "Existe un MISC\n",
      "Europa LOC\n",
      "América del Norte LOC\n"
     ]
    }
   ],
   "source": [
    "for palabra in doc.ents[:15]:\n",
    "    print(palabra.text,palabra.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nos nace la duda con alguna etiqueta podemos hacer lo siguiente:\n",
    "*De esta manera nos mostrara cual es el significado de dicha etiqueta*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Miscellaneous entities, e.g. events, nationalities, products or works of art'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('MISC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similaridad\n",
    "\n",
    "spaCy puede comparar dos objetos, y hacer una predicción de cuán similares son . Predecir similitudes es útil para crear sistemas de recomendación o marcar duplicados. Por ejemplo, puede sugerir un contenido de usuario similar a lo que están viendo actualmente, o etiquetar un ticket de soporte como un duplicado si es muy similar a uno ya existente. https://spacy.io/usage/vectors-similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1=nlp('listo')\n",
    "ex2=nlp('inteligente')\n",
    "ex3=nlp('gato')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24447840411896346"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex1.similarity(ex2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4884264154757697"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex1.similarity(ex3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se realiza una comparación de conceptos de forma anidada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('gato', 'gato')  tienen una similaridad de:  1.0\n",
      "('gato', 'perro')  tienen una similaridad de:  0.26863766\n",
      "('gato', 'leon')  tienen una similaridad de:  0.028337544\n",
      "('gato', 'pez')  tienen una similaridad de:  0.08411365\n",
      "('gato', 'lobo')  tienen una similaridad de:  0.31157687\n",
      "('gato', 'delfin')  tienen una similaridad de:  0.043116048\n",
      "('gato', 'aguila')  tienen una similaridad de:  -0.09820114\n",
      "('perro', 'gato')  tienen una similaridad de:  0.26863766\n",
      "('perro', 'perro')  tienen una similaridad de:  1.0\n",
      "('perro', 'leon')  tienen una similaridad de:  0.0067966576\n",
      "('perro', 'pez')  tienen una similaridad de:  0.41379523\n",
      "('perro', 'lobo')  tienen una similaridad de:  0.26735717\n",
      "('perro', 'delfin')  tienen una similaridad de:  0.36840555\n",
      "('perro', 'aguila')  tienen una similaridad de:  0.09055745\n",
      "('leon', 'gato')  tienen una similaridad de:  0.028337544\n",
      "('leon', 'perro')  tienen una similaridad de:  0.0067966576\n",
      "('leon', 'leon')  tienen una similaridad de:  1.0\n",
      "('leon', 'pez')  tienen una similaridad de:  -0.040517125\n",
      "('leon', 'lobo')  tienen una similaridad de:  0.096036844\n",
      "('leon', 'delfin')  tienen una similaridad de:  -0.010851557\n",
      "('leon', 'aguila')  tienen una similaridad de:  0.19522992\n",
      "('pez', 'gato')  tienen una similaridad de:  0.08411365\n",
      "('pez', 'perro')  tienen una similaridad de:  0.41379523\n",
      "('pez', 'leon')  tienen una similaridad de:  -0.040517125\n",
      "('pez', 'pez')  tienen una similaridad de:  1.0\n",
      "('pez', 'lobo')  tienen una similaridad de:  0.20183012\n",
      "('pez', 'delfin')  tienen una similaridad de:  0.33669624\n",
      "('pez', 'aguila')  tienen una similaridad de:  0.008457191\n",
      "('lobo', 'gato')  tienen una similaridad de:  0.31157687\n",
      "('lobo', 'perro')  tienen una similaridad de:  0.26735717\n",
      "('lobo', 'leon')  tienen una similaridad de:  0.096036844\n",
      "('lobo', 'pez')  tienen una similaridad de:  0.20183012\n",
      "('lobo', 'lobo')  tienen una similaridad de:  1.0\n",
      "('lobo', 'delfin')  tienen una similaridad de:  0.050943382\n",
      "('lobo', 'aguila')  tienen una similaridad de:  0.10440019\n",
      "('delfin', 'gato')  tienen una similaridad de:  0.043116048\n",
      "('delfin', 'perro')  tienen una similaridad de:  0.36840555\n",
      "('delfin', 'leon')  tienen una similaridad de:  -0.010851557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('delfin', 'pez')  tienen una similaridad de:  0.33669624\n",
      "('delfin', 'lobo')  tienen una similaridad de:  0.050943382\n",
      "('delfin', 'delfin')  tienen una similaridad de:  1.0\n",
      "('delfin', 'aguila')  tienen una similaridad de:  -0.04175953\n",
      "('aguila', 'gato')  tienen una similaridad de:  -0.09820114\n",
      "('aguila', 'perro')  tienen una similaridad de:  0.09055745\n",
      "('aguila', 'leon')  tienen una similaridad de:  0.19522992\n",
      "('aguila', 'pez')  tienen una similaridad de:  0.008457191\n",
      "('aguila', 'lobo')  tienen una similaridad de:  0.10440019\n",
      "('aguila', 'delfin')  tienen una similaridad de:  -0.04175953\n",
      "('aguila', 'aguila')  tienen una similaridad de:  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "lista=nlp('gato perro leon pez lobo delfin aguila')\n",
    "for w1 in lista:\n",
    "    for w2 in lista:\n",
    "        print((w1.text,w2.text),' tienen una similaridad de: ',w1.similarity(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lo mismo en su version comprimida*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('gato', 'gato', 1.0),\n",
       " ('gato', 'perro', 0.26863766),\n",
       " ('gato', 'leon', 0.028337544),\n",
       " ('gato', 'pez', 0.08411365),\n",
       " ('gato', 'lobo', 0.31157687),\n",
       " ('gato', 'delfin', 0.043116048),\n",
       " ('gato', 'aguila', -0.09820114),\n",
       " ('perro', 'gato', 0.26863766),\n",
       " ('perro', 'perro', 1.0),\n",
       " ('perro', 'leon', 0.0067966576),\n",
       " ('perro', 'pez', 0.41379523),\n",
       " ('perro', 'lobo', 0.26735717),\n",
       " ('perro', 'delfin', 0.36840555),\n",
       " ('perro', 'aguila', 0.09055745),\n",
       " ('leon', 'gato', 0.028337544),\n",
       " ('leon', 'perro', 0.0067966576),\n",
       " ('leon', 'leon', 1.0),\n",
       " ('leon', 'pez', -0.040517125),\n",
       " ('leon', 'lobo', 0.096036844),\n",
       " ('leon', 'delfin', -0.010851557),\n",
       " ('leon', 'aguila', 0.19522992),\n",
       " ('pez', 'gato', 0.08411365),\n",
       " ('pez', 'perro', 0.41379523),\n",
       " ('pez', 'leon', -0.040517125),\n",
       " ('pez', 'pez', 1.0),\n",
       " ('pez', 'lobo', 0.20183012),\n",
       " ('pez', 'delfin', 0.33669624),\n",
       " ('pez', 'aguila', 0.008457191),\n",
       " ('lobo', 'gato', 0.31157687),\n",
       " ('lobo', 'perro', 0.26735717),\n",
       " ('lobo', 'leon', 0.096036844),\n",
       " ('lobo', 'pez', 0.20183012),\n",
       " ('lobo', 'lobo', 1.0),\n",
       " ('lobo', 'delfin', 0.050943382),\n",
       " ('lobo', 'aguila', 0.10440019),\n",
       " ('delfin', 'gato', 0.043116048),\n",
       " ('delfin', 'perro', 0.36840555),\n",
       " ('delfin', 'leon', -0.010851557),\n",
       " ('delfin', 'pez', 0.33669624),\n",
       " ('delfin', 'lobo', 0.050943382),\n",
       " ('delfin', 'delfin', 1.0),\n",
       " ('delfin', 'aguila', -0.04175953),\n",
       " ('aguila', 'gato', -0.09820114),\n",
       " ('aguila', 'perro', 0.09055745),\n",
       " ('aguila', 'leon', 0.19522992),\n",
       " ('aguila', 'pez', 0.008457191),\n",
       " ('aguila', 'lobo', 0.10440019),\n",
       " ('aguila', 'delfin', -0.04175953),\n",
       " ('aguila', 'aguila', 1.0)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi_lista=[(w1.text,w2.text,w1.similarity(w2)) for w1 in lista for w2 in lista]\n",
    "mi_lista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "\n",
    "las stopwords es el nombre que reciben las palabras sin significado como artículos, pronombres, preposiciones, etc. que son filtradas antes o después del procesamiento de datos en lenguaje natural (texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.es.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fuimos', 'al', 'voy', 'trabajo', 'hablan', 'está', 'verdadero', 'este', 'intentas', 'tiempo', 'buen', 'suyo', 'mío', 'sabe', 'si', 'verdadera', 'afirmó', 'mias', 'puedo', 'toda', 'hecho', 'ultimo', 'pais', 'pocos', 'hago', 'adelante', 'apenas', 'añadió', 'nuestras', 'fui']\n"
     ]
    }
   ],
   "source": [
    "# para observar las palabras dentro de la lista lo unico que se tiene que hacer eso imprimir\n",
    "print(list(STOP_WORDS)[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aproximadamente Spacy tiene 551 palabras en Stop Words\n",
    "len(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para verificar si una palabra es stopword\n",
    "nlp.vocab['estados'].is_stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para agregar mas stopwords\n",
    "STOP_WORDS.add(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[\"y\"].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código ayuda a filtrar las palabras de un texto y de esta manera determinar las palabras que han pasado dicha regla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      ", accidente, Chernóbil, accidente, nuclear, sucedido, 26, abril, 1986, central, nuclear, Vladímir, Ilich, Lenin, ,, ubicada, norte, Ucrania, ,, pertenecía, a, Unión, Repúblicas, Socialistas, Soviéticas, ,, a, 3, km, ciudad, Prípiat, ,, a, 18,  , km, ciudad, Chernóbil, y, a, 17, km, frontera, Bielorrusia, ., \n",
      "\n",
      ", Considerado, ,, accidente, nuclear, Fukushima, I, Japón, 2011, ,, grave, Escala, Internacional, Accidentes, Nucleares, (, accidente, ,, nivel, 7, ), ,, y, suele, incluido, desastres, medioambientales, historia, ., \n",
      "\n",
      ", causas, y, desarrollo, accidente, objeto, controversias, ., consenso, venía, realizando, prueba, requería, reducir, potencia, ,, produjeron, serie, desequilibrios, reactor, 4, central, nuclear, ,, desembocaron, sobrecalentamiento]\n"
     ]
    }
   ],
   "source": [
    "#Para filtrar stopwords\n",
    "\n",
    "lista_texto_filtrado=[palabra for palabra in doc if palabra.is_stop==False]\n",
    "\n",
    "print(lista_texto_filtrado[:100])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un metodo para limpiar esos espacios entre elementos es buscar los saltos de linea y remplazarlos y mediante el metodo join lo que hace es que a la lista de estrings anteriormente mostrada la introduce en una lista de un solo string pero sin los espacios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' accidente Chernóbil accidente nuclear sucedido 26 abril 1986 central nuclear Vladímir Ilich Lenin , ubicada norte Ucrania , pertenecía a Unión Repúblicas Socialistas Soviéticas , a 3 km ciudad Prípiat , a 18   km ciudad Chernóbil y a 17 km frontera Bielorrusia .  Considerado , accidente nuclear Fukushima I Japón 2011 , grave Escala Internacional Accidentes Nucleares ( accidente , nivel 7 ) , y suele incluido desastres medioambientales historia .'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#De manera similar\n",
    "' '.join([palabra.text for palabra in doc[:120] if palabra.is_stop==False]).replace('\\n','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fragmentación\n",
    "\n",
    "Doc.noun_chunks es una iteración sobre la frase sustantiva basica es decir divide el documento en estas frases, despues mediante el for recorremos estos elementos y vamos imprimiendo el elemento raiz seguido de sus conectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accidente su conector es:  accidente\n",
      "Chernóbil su conector es:  accidente\n",
      "accidente su conector es:  accidente\n",
      "abril su conector es:  26\n",
      "central su conector es:  sucedido\n",
      "Vladímir su conector es:  central\n",
      "norte su conector es:  ubicada\n",
      "Ucrania su conector es:  norte\n",
      "que su conector es:  pertenecía\n",
      "momento su conector es:  pertenecía\n",
      "Unión su conector es:  pertenecía\n",
      "km su conector es:  pertenecía\n",
      "ciudad su conector es:  km\n",
      "Prípiat su conector es:  ciudad\n",
      "ciudad su conector es:  km\n",
      "Chernóbil su conector es:  ciudad\n",
      "km su conector es:  ciudad\n",
      "frontera su conector es:  km\n",
      "Bielorrusia su conector es:  frontera\n",
      "accidente su conector es:  Considerado\n",
      "Fukushima su conector es:  accidente\n",
      "Japón su conector es:  accidente\n",
      "2011 su conector es:  accidente\n",
      "Escala su conector es:  grave\n",
      "accidente su conector es:  Escala\n",
      "nivel su conector es:  accidente\n",
      "desastres su conector es:  incluido\n",
      "historia su conector es:  desastres\n",
      "causas su conector es:  objeto\n",
      "desarrollo su conector es:  causas\n",
      "accidente su conector es:  causas\n",
      "objeto su conector es:  objeto\n",
      "controversias su conector es:  objeto\n",
      "consenso su conector es:  Existe\n",
      "que su conector es:  realizando\n",
      "día su conector es:  realizando\n",
      "se su conector es:  realizando\n",
      "prueba su conector es:  realizando\n",
      "que su conector es:  requería\n",
      "potencia su conector es:  reducir\n",
      "cual su conector es:  produjeron\n",
      "se su conector es:  produjeron\n",
      "serie su conector es:  produjeron\n",
      "desequilibrios su conector es:  serie\n",
      "reactor su conector es:  serie\n",
      "central su conector es:  reactor\n",
      "que su conector es:  desembocaron\n",
      "sobrecalentamiento su conector es:  desembocaron\n",
      "núcleo su conector es:  sobrecalentamiento\n",
      "reactor su conector es:  núcleo\n",
      "explosiones su conector es:  núcleo\n",
      "incendio su conector es:  seguidas\n",
      "que su conector es:  volaron\n",
      "tapa su conector es:  volaron\n",
      "reactor su conector es:  tapa\n",
      "1200 su conector es:  toneladas\n",
      "toneladas su conector es:  reactor\n",
      "cantidades su conector es:  expulsaron\n",
      "materiales su conector es:  cantidades\n",
      "atmósfera su conector es:  cantidades\n",
      "nube su conector es:  formando\n",
      "que su conector es:  extendió\n",
      "se su conector es:  extendió\n",
      "Europa su conector es:  extendió\n",
      "América su conector es:  Europa\n",
      "cantidad su conector es:  mayor\n",
      "dióxido su conector es:  cantidad\n",
      "uranio su conector es:  dióxido\n",
      "carburo su conector es:  dióxido\n",
      "boro su conector es:  carburo\n",
      "europio su conector es:  óxido\n",
      "erbio su conector es:  europio\n",
      "aleaciones su conector es:  dióxido\n",
      "circonio su conector es:  aleaciones\n",
      "grafito su conector es:  circonio\n",
      "materiales su conector es:  aleaciones\n",
      "que su conector es:  estimó\n",
      "se su conector es:  estimó\n",
      "unas su conector es:  500\n",
      "veces su conector es:  mayor\n",
      "bomba su conector es:  liberado\n",
      "Hiroshima su conector es:  arrojada\n",
      "1945 su conector es:  arrojada\n",
      "muerte su conector es:  causó\n",
      "personas su conector es:  muerte\n",
      "semanas su conector es:  causó\n",
      "Gobierno su conector es:  llevó\n",
      "Unión su conector es:  Gobierno\n",
      "evacuación su conector es:  llevó\n",
      "urgencia su conector es:  evacuación\n",
      "personas su conector es:  evacuación\n",
      "alarma su conector es:  provocando\n",
      "radiactividad su conector es:  detectarse\n",
      "países su conector es:  detectarse\n",
      "Europa su conector es:  países\n",
      "accidente su conector es:  Después\n",
      "se su conector es:  inició\n",
      "proceso su conector es:  inició\n",
      "descontaminación su conector es:  proceso\n",
      "contención su conector es:  descontaminación\n",
      "mitigación su conector es:  descontaminación\n",
      "que su conector es:  desempeñaron\n",
      "personas su conector es:  desempeñaron\n",
      "zonas su conector es:  denominadas\n",
      "lugar su conector es:  al\n",
      "accidente su conector es:  denominadas\n",
      "se su conector es:  aisló\n",
      "área su conector es:  aisló\n",
      "km su conector es:  área\n",
      "radio su conector es:  km\n",
      "central su conector es:  alrededor\n",
      "zona su conector es:  conocida\n",
      "alienación su conector es:  zona\n",
      "que su conector es:  sigue\n",
      "parte su conector es:  Solo\n",
      "liquidadores su conector es:  parte\n",
      "se su conector es:  vieron\n",
      "índices su conector es:  expuestos\n",
      "radiactividad su conector es:  índices\n",
      "empleados su conector es:  murieron\n",
      "planta su conector es:  empleados\n",
      "consecuencia su conector es:  murieron\n",
      "explosión su conector es:  consecuencia\n",
      "otros su conector es:  29\n",
      "meses su conector es:  fallecieron\n",
      "Unas su conector es:  1000\n",
      "1000 su conector es:  personas\n",
      "personas su conector es:  recibieron\n",
      "dosis su conector es:  recibieron\n",
      "radiación su conector es:  dosis\n",
      "día su conector es:  después\n",
      "accidente su conector es:  después\n",
      "personas su conector es:  recibieron\n",
      "mSv su conector es:  recibieron\n",
      "mSv su conector es:  recibieron\n",
      "algunos su conector es:  500\n",
      "mSv su conector es:  mSv\n",
      "total su conector es:  En\n",
      "personas su conector es:  recibieron\n",
      "dosis su conector es:  recibieron\n",
      "radiación su conector es:  dosis\n",
      "trabajos su conector es:  dosis\n",
      "descontaminación su conector es:  trabajos\n",
      "accidente su conector es:  posteriores\n",
      "personas su conector es:  000\n",
      "áreas su conector es:  vivieron\n",
      "áreas su conector es:  000\n",
      "trabajos su conector es:  existen\n",
      "incidencia su conector es:  trabajos\n",
      "accidente su conector es:  trabajos\n",
      "mortalidad su conector es:  accidente\n",
      "población su conector es:  mortalidad\n",
      "negociaciones su conector es:  financió\n",
      "Gobierno su conector es:  negociaciones\n",
      "comunidad su conector es:  financió\n",
      "costes su conector es:  financió\n",
      "cierre su conector es:  costes\n",
      "central su conector es:  cierre\n",
      "diciembre su conector es:  15\n",
      "accidente su conector es:  después\n",
      "se su conector es:  construyó\n",
      "sarcófago su conector es:  construyó\n",
      "reactor su conector es:  cubrir\n",
      "interior su conector es:  aislar\n",
      "exterior su conector es:  interior\n",
      "que su conector es:  vio\n",
      "se su conector es:  vio\n",
      "paso su conector es:  degradado\n",
      "tiempo su conector es:  paso\n",
      "fenómenos su conector es:  degradado\n",
      "dificultades su conector es:  construyó\n",
      "construirlo su conector es:  dificultades\n",
      "ambiente su conector es:  dificultades\n",
      "radiación su conector es:  ambiente\n",
      "que su conector es:  corría\n",
      "riesgo su conector es:  corría\n",
      "degradarse su conector es:  riesgo\n",
      "2004 su conector es:  inició\n",
      "se su conector es:  inició\n",
      "construcción su conector es:  inició\n",
      "sarcófago su conector es:  construcción\n",
      "reactor su conector es:  sarcófago\n",
      "resto su conector es:  inactivos\n",
      "reactores su conector es:  resto\n",
      "central su conector es:  reactores\n",
      "noviembre su conector es:  inauguró\n",
      "años su conector es:  después\n",
      "tragedia su conector es:  después\n",
      "se su conector es:  inauguró\n",
      "sarcófago su conector es:  inauguró\n",
      "que su conector es:  denominó\n",
      "se su conector es:  denominó\n",
      "sarcófago su conector es:  denominó\n",
      "NSC su conector es:  sarcófago\n",
      "siglas su conector es:  NSC\n",
      "inglés su conector es:  siglas\n",
      "estructura su conector es:  sarcófago\n",
      "fecha su conector es:  construida\n",
      "mundo su conector es:  fecha\n",
      "forma su conector es:  construida\n",
      "arco su conector es:  forma\n",
      "metros su conector es:  arco\n",
      "ancho su conector es:  150\n",
      "largo su conector es:  256\n",
      "toneladas su conector es:  sarcófago\n",
      "peso su conector es:  toneladas\n",
      "Se su conector es:  construyó\n",
      "metros su conector es:  construyó\n",
      "reactor su conector es:  metros\n",
      "se su conector es:  ubicó\n",
      "él su conector es:  ubicó\n",
      "desplazándolo su conector es:  él\n",
      "sistema su conector es:  desplazándolo\n",
      "raíles su conector es:  sistema\n",
      "Se su conector es:  construyó\n",
      "características su conector es:  construyó\n",
      "que su conector es:  dieron\n",
      "le su conector es:  dieron\n",
      "durabilidad su conector es:  dieron\n",
      "años su conector es:  estimada\n",
      "coste su conector es:  millones\n",
      "estructura su conector es:  coste\n",
      "1500 su conector es:  millones\n",
      "millones su conector es:  millones\n",
      "euros su conector es:  millones\n",
      "Banco su conector es:  financiado\n",
      "Desarrollo su conector es:  Reconstrucción\n",
      "BERD su conector es:  Banco\n",
      "colaboración su conector es:  financiado\n",
      "países su conector es:  colaboración\n",
      "que su conector es:  aportaron\n",
      "1417 su conector es:  millones\n",
      "millones su conector es:  aportaron\n",
      "euros su conector es:  millones\n",
      "empresa su conector es:  construido\n",
      "Novarka su conector es:  empresa\n",
      "estructura su conector es:  equipada\n",
      "grúas su conector es:  equipada\n",
      "distancia su conector es:  controladas\n",
      "objetivo su conector es:  con\n",
      "estructura su conector es:  desmontando\n"
     ]
    }
   ],
   "source": [
    "for palabra in doc.noun_chunks:\n",
    "    print(palabra.root.text,'su conector es: ', palabra.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "El accidente de Chernóbil fue un accidente nuclear sucedido el 26 de abril de 1986 en la central nuclear Vladímir Ilich Lenin, ubicada en el norte de Ucrania, que en ese momento pertenecía a la Unión de Repúblicas Socialistas Soviéticas, a 3 km de la ciudad de Prípiat, a 18  km de la ciudad de Chernóbil y a 17 km de la frontera con Bielorrusia.\n",
      "\n",
      "\n",
      "Considerado, junto con el accidente nuclear de Fukushima I en Japón en 2011, como el más grave en la Escala Internacional de Accidentes Nucleares (accidente mayor, nivel 7), y suele ser incluido entre los grandes desastres medioambientales de la historia.\n",
      "\n",
      "\n",
      "Las causas y desarrollo del accidente son objeto de controversias.\n",
      "Existe un consenso general en que desde el día anterior se venía realizando una prueba que requería reducir la potencia, durante la cual se produjeron una serie de desequilibrios en el reactor 4 de esta central nuclear, que desembocaron en el sobrecalentamiento descontrolado del núcleo del reactor nuclear y en una o dos explosiones sucesivas, seguidas de un incendio generalizado, que volaron la tapa del reactor de 1200 toneladas y expulsaron grandes cantidades de materiales radiactivos a la atmósfera, formando una nube radiactiva que se extendió por Europa y América del Norte.\n",
      "\n",
      "\n",
      "La cantidad de dióxido de uranio, carburo de boro, óxido de europio, erbio, aleaciones de circonio y grafito expulsados,6​ materiales radiactivos y/o tóxicos, que se estimó fue unas 500 veces mayor que el liberado por la bomba atómica arrojada en Hiroshima en 1945, causó la muerte de 31 personas en las siguientes dos semanas y llevó al Gobierno de la Unión Soviética a la evacuación de urgencia de 116 000 personas, provocando una alarma internacional al detectarse radiactividad en al menos 13 países de Europa central y oriental.\n",
      "\n",
      "\n",
      "Después del accidente, se inició un proceso masivo de descontaminación, contención y mitigación que desempeñaron aproximadamente 600 000 personas denominadas liquidadores en las zonas circundantes al lugar del accidente y se aisló un área de 30 km de radio alrededor de la central nuclear conocida como zona de alienación, que sigue aún vigente.\n",
      "Solo una pequeña parte de los liquidadores se vieron expuestos a altos índices de radiactividad.\n",
      "\n",
      "\n",
      "Dos empleados de la planta murieron como consecuencia directa de la explosión y otros 29 fallecieron en los tres meses siguientes.\n",
      "Unas 1000 personas recibieron grandes dosis de radiación durante el primer día después del accidente, 200 000 personas recibieron alrededor de 100 mSv, 20 000 cerca de 250 mSv y algunos 500 mSv.\n",
      "En total, 600 000 personas recibieron dosis de radiación por los trabajos de descontaminación posteriores al accidente.\n",
      "5 000 000 de personas vivieron en áreas contaminadas y 400 000 en áreas gravemente contaminadas.\n",
      "Hasta hoy no existen trabajos concluyentes sobre la incidencia real, y no teórica, de este accidente en la mortalidad de la población.\n",
      "\n",
      "\n",
      "Tras prolongadas negociaciones con el Gobierno ucraniano, la comunidad internacional financió los costes del cierre definitivo de la central, completado el 15 de diciembre de 2000.\n",
      "Inmediatamente después del accidente se construyó un «sarcófago», para cubrir el reactor y aislar el interior del exterior, que se vio degradado con el paso del tiempo por diversos fenómenos naturales, y por las dificultades de construirlo en un ambiente de alta radiación, por lo que corría riesgo de degradarse seriamente.\n",
      "En 2004, se inició la construcción de un nuevo sarcófago para el reactor.\n",
      "El resto de reactores de la central están inactivos.\n",
      "\n",
      "\n",
      "En noviembre de 2016, treinta años después de la tragedia, se inauguró un nuevo sarcófago, al que se denominó «nuevo sarcófago seguro» (NSC, por sus siglas en inglés), una estructura móvil, la mayor construida hasta la fecha en el mundo, en forma de arco de 110 metros de alto, 150 de ancho y 256 de largo y más de 30 000 toneladas de peso.\n",
      "Se construyó a 180 metros del reactor y luego se ubicó sobre él, desplazándolo mediante un sofisticado sistema de raíles.\n",
      "Se construyó con características que le dieron una durabilidad estimada de más de cien años.\n",
      "El coste final de la estructura fue de 1500 millones de euros, financiado por el Banco Europeo para la Reconstrucción y el Desarrollo (BERD), junto a la colaboración de 28 países que aportaron 1417 millones de euros, y construido por la empresa francesa Novarka.\n",
      "La estructura está equipada con grúas controladas a distancia con el objetivo de ir desmontando la antigua estructura.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#si deseas dividir el documento en oraciones solo se tiene que usar el comando sents\n",
    "for oracion in doc.sents:\n",
    "    print(oracion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encontrar las palabras mas comunes\n",
    "\n",
    "para encontrar las palabras mas comunes usaremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accidente', 'accidente', 'abril', 'central', 'norte', 'km', 'ciudad', 'ciudad', 'km', 'frontera', 'accidente', '2011', 'accidente', 'nivel', 'desastres', 'historia', 'causas', 'desarrollo', 'accidente', 'objeto', 'controversias', 'consenso', 'prueba', 'potencia', 'serie', 'desequilibrios', 'reactor', 'central', 'sobrecalentamiento', 'núcleo', 'reactor', 'explosiones', 'incendio', 'tapa', 'reactor', '1200', 'toneladas', 'cantidades', 'materiales', 'atmósfera', 'nube', 'cantidad', 'dióxido', 'uranio', 'carburo', 'boro', 'europio', 'erbio', 'aleaciones', 'circonio']\n"
     ]
    }
   ],
   "source": [
    "#Para nombres mas comunes\n",
    "nombres=[w.text for w in nlp(texto) if w.is_stop!=True and w.is_punct!=True and w.pos_=='NOUN']\n",
    "print(nombres[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter(nombres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colocamos los 5 sustantivos mas comunes y el numero de veces que es repetido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('accidente', 11), ('personas', 7), ('reactor', 6), ('central', 5), ('sarcófago', 4)]\n"
     ]
    }
   ],
   "source": [
    "nombres_comunes=word_freq.most_common(5)\n",
    "print(nombres_comunes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pertenecía', 'incluido', 'Existe', 'venía', 'realizando', 'requería', 'reducir', 'produjeron', 'desembocaron', 'volaron', 'expulsaron', 'formando', 'extendió', 'estimó', 'causó', 'llevó', 'provocando', 'detectarse', 'inició', 'desempeñaron', 'aisló', 'sigue', 'vieron', 'murieron', 'fallecieron', 'recibieron', 'recibieron', 'recibieron', 'vivieron', 'existen', 'teórica', 'financió', 'construyó', 'cubrir', 'aislar', 'vio', 'corría', 'inició', 'inauguró', 'denominó', 'construyó', 'ubicó', 'construyó', 'dieron', 'aportaron', 'desmontando']\n"
     ]
    }
   ],
   "source": [
    "#Para verbos\n",
    "verbos=[w.text for w in nlp(texto) if w.is_punct!=True and w.pos_=='VERB']\n",
    "print(verbos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('recibieron', 3), ('construyó', 3), ('inició', 2), ('pertenecía', 1), ('incluido', 1)]\n"
     ]
    }
   ],
   "source": [
    "word_freq2 = Counter(verbos)\n",
    "verbos_comunes=word_freq2.most_common(5)\n",
    "print(verbos_comunes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spacy env",
   "language": "python",
   "name": "spacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
